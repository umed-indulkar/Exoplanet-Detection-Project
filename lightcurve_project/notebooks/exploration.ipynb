{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light Curve Analysis Exploration\n",
    "\n",
    "This notebook demonstrates the complete workflow for processing, visualizing, and extracting features from light curves.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Import our modules\n",
    "from data_loader import load_npz_curve, preprocess_lightcurve\n",
    "from visualization import (plot_lightcurve, plot_folded_curve, \n",
    "                          plot_feature_distribution, plot_comprehensive_analysis)\n",
    "from feature_extraction import extract_features\n",
    "from feature_pruning import manual_prune, analyze_feature_importance\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create Synthetic Light Curve Data\n",
    "\n",
    "For demonstration, we'll create a synthetic light curve with transit signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic light curve with transits\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_points = 2000\n",
    "time_span = 100  # days\n",
    "period = 8.5  # days\n",
    "transit_depth = 0.015\n",
    "transit_duration = 0.3  # days\n",
    "noise_level = 0.003\n",
    "\n",
    "# Generate time series\n",
    "time = np.sort(np.random.uniform(0, time_span, n_points))\n",
    "flux = np.ones_like(time)\n",
    "\n",
    "# Add transit signals\n",
    "phase = (time / period) % 1.0\n",
    "transit_phase = 0.2  # Transit occurs at phase 0.2\n",
    "\n",
    "# Create transit shape (simplified box model)\n",
    "for i, ph in enumerate(phase):\n",
    "    phase_diff = min(abs(ph - transit_phase), abs(ph - transit_phase + 1), abs(ph - transit_phase - 1))\n",
    "    if phase_diff < (transit_duration / period / 2):\n",
    "        # Simple box transit\n",
    "        flux[i] -= transit_depth\n",
    "\n",
    "# Add stellar variability (sinusoidal)\n",
    "stellar_period = 25.0  # rotation period\n",
    "stellar_amplitude = 0.008\n",
    "flux += stellar_amplitude * np.sin(2 * np.pi * time / stellar_period)\n",
    "\n",
    "# Add noise\n",
    "flux += np.random.normal(0, noise_level, len(flux))\n",
    "flux_err = np.full_like(flux, noise_level)\n",
    "\n",
    "print(f\"Created synthetic light curve:\")\n",
    "print(f\"  Time span: {time_span} days\")\n",
    "print(f\"  Data points: {n_points}\")\n",
    "print(f\"  Transit period: {period} days\")\n",
    "print(f\"  Transit depth: {transit_depth*100:.1f}%\")\n",
    "print(f\"  Noise level: {noise_level*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data\n",
    "\n",
    "In a real scenario, you would load from an .npz file. Here we'll use our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create light curve data structure\n",
    "lc_data = {\n",
    "    'time': time,\n",
    "    'flux': flux,\n",
    "    'flux_err': flux_err\n",
    "}\n",
    "\n",
    "print(\"Raw light curve statistics:\")\n",
    "print(f\"  Mean flux: {np.mean(flux):.6f}\")\n",
    "print(f\"  Flux std: {np.std(flux):.6f}\")\n",
    "print(f\"  Flux range: {np.max(flux) - np.min(flux):.6f}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"\\nApplying preprocessing...\")\n",
    "processed_lc = preprocess_lightcurve(lc_data, apply_preprocessing=True)\n",
    "\n",
    "print(\"\\nProcessed light curve statistics:\")\n",
    "print(f\"  Mean flux: {np.mean(processed_lc['flux']):.6f}\")\n",
    "print(f\"  Flux std: {np.std(processed_lc['flux']):.6f}\")\n",
    "print(f\"  Data points: {len(processed_lc['time'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Light Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raw light curve\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Raw data\n",
    "ax1.scatter(lc_data['time'], lc_data['flux'], alpha=0.6, s=8, label='Raw data')\n",
    "ax1.set_xlabel('Time (days)')\n",
    "ax1.set_ylabel('Flux')\n",
    "ax1.set_title('Raw Light Curve')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Processed data\n",
    "ax2.scatter(processed_lc['time'], processed_lc['flux'], alpha=0.6, s=8, \n",
    "           color='orange', label='Processed data')\n",
    "ax2.set_xlabel('Time (days)')\n",
    "ax2.set_ylabel('Normalized Flux')\n",
    "ax2.set_title('Processed Light Curve')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Phase-Folded Light Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot folded light curve using known period\n",
    "fig, axes = plot_folded_curve(\n",
    "    processed_lc['time'], \n",
    "    processed_lc['flux'], \n",
    "    period,\n",
    "    flux_err=processed_lc['flux_err'],\n",
    "    title=f\"Phase-Folded Light Curve (P={period} days)\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Extraction\n",
    "\n",
    "Extract comprehensive features from the light curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "print(\"Extracting features...\")\n",
    "features_df = extract_features(\n",
    "    processed_lc['time'], \n",
    "    processed_lc['flux'], \n",
    "    processed_lc['flux_err']\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted {len(features_df.columns)} features\")\n",
    "print(f\"Feature categories:\")\n",
    "\n",
    "# Group features by category\n",
    "categories = {\n",
    "    'Basic Stats': [c for c in features_df.columns if any(x in c for x in ['mean', 'std', 'var', 'median'])],\n",
    "    'Time Domain': [c for c in features_df.columns if any(x in c for x in ['autocorr', 'diff', 'trend'])],\n",
    "    'Frequency': [c for c in features_df.columns if any(x in c for x in ['spectral', 'ls_', 'freq'])],\n",
    "    'Variability': [c for c in features_df.columns if any(x in c for x in ['amplitude', 'stetson', 'rms'])],\n",
    "    'Transit': [c for c in features_df.columns if any(x in c for x in ['transit', 'dip'])],\n",
    "}\n",
    "\n",
    "for cat, features in categories.items():\n",
    "    print(f\"  {cat}: {len(features)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Examine Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some key features\n",
    "key_features = [\n",
    "    'mean', 'std', 'amplitude', 'skewness', 'kurtosis',\n",
    "    'ls_peak_power', 'ls_peak_period', 'spectral_entropy',\n",
    "    'stetson_j', 'von_neumann_ratio',\n",
    "    'deepest_dip_depth', 'n_dips'\n",
    "]\n",
    "\n",
    "print(\"Key Feature Values:\")\n",
    "print(\"=\" * 50)\n",
    "for feature in key_features:\n",
    "    if feature in features_df.columns:\n",
    "        value = features_df[feature].iloc[0]\n",
    "        if isinstance(value, float) and not np.isnan(value):\n",
    "            print(f\"{feature:25s}: {value:10.6f}\")\n",
    "        else:\n",
    "            print(f\"{feature:25s}: {str(value):>10s}\")\n",
    "    else:\n",
    "        print(f\"{feature:25s}: {'Not found':>10s}\")\n",
    "\n",
    "# Check if we detected the period correctly\n",
    "if 'ls_peak_period' in features_df.columns:\n",
    "    detected_period = features_df['ls_peak_period'].iloc[0]\n",
    "    if not np.isnan(detected_period):\n",
    "        print(f\"\\nPeriod Detection:\")\n",
    "        print(f\"  True period: {period:.2f} days\")\n",
    "        print(f\"  Detected period: {detected_period:.2f} days\")\n",
    "        print(f\"  Error: {abs(detected_period - period):.2f} days ({abs(detected_period - period)/period*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature distributions\n",
    "fig, axes = plot_feature_distribution(features_df, max_features=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance (by variance)\n",
    "importance = analyze_feature_importance(features_df, method='variance')\n",
    "\n",
    "print(\"Top 20 Most Variable Features:\")\n",
    "print(\"=\" * 50)\n",
    "for i, (feature, var) in enumerate(importance.head(20).items(), 1):\n",
    "    print(f\"{i:2d}. {feature:30s}: {var:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Selection and Pruning\n",
    "\n",
    "Select a subset of features for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select interesting features manually\n",
    "selected_features = [\n",
    "    # Basic statistics\n",
    "    'mean', 'std', 'amplitude', 'skewness', 'kurtosis',\n",
    "    \n",
    "    # Variability\n",
    "    'stetson_j', 'von_neumann_ratio', 'welch_stetson',\n",
    "    \n",
    "    # Periodicity\n",
    "    'ls_peak_power', 'ls_peak_period', 'autocorr_lag_1',\n",
    "    \n",
    "    # Frequency domain\n",
    "    'spectral_entropy', 'spectral_centroid', 'dominant_freq',\n",
    "    \n",
    "    # Transit features\n",
    "    'deepest_dip_depth', 'n_dips', 'transit_ingress_slope',\n",
    "    \n",
    "    # Shape features\n",
    "    'asymmetry_ratio', 'frac_above_2sigma', 'max_run_above_mean'\n",
    "]\n",
    "\n",
    "# Filter to features that actually exist\n",
    "selected_features = [f for f in selected_features if f in features_df.columns]\n",
    "\n",
    "print(f\"Selected {len(selected_features)} features for analysis:\")\n",
    "for feature in selected_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Create pruned dataset\n",
    "pruned_features = manual_prune(features_df, selected_features)\n",
    "print(f\"\\nPruned dataset shape: {pruned_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV files\n",
    "features_df.to_csv('../output/all_features.csv', index=False)\n",
    "pruned_features.to_csv('../output/selected_features.csv', index=False)\n",
    "\n",
    "print(\"Results saved:\")\n",
    "print(f\"  All features: ../output/all_features.csv ({len(features_df.columns)} features)\")\n",
    "print(f\"  Selected features: ../output/selected_features.csv ({len(selected_features)} features)\")\n",
    "\n",
    "# Also save the light curve data\n",
    "lc_df = pd.DataFrame({\n",
    "    'time': processed_lc['time'],\n",
    "    'flux': processed_lc['flux'],\n",
    "    'flux_err': processed_lc['flux_err']\n",
    "})\n",
    "lc_df.to_csv('../output/processed_lightcurve.csv', index=False)\n",
    "print(f\"  Light curve: ../output/processed_lightcurve.csv ({len(lc_df)} points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook demonstrated the complete workflow:\n",
    "\n",
    "1. **Data Creation**: Generated synthetic transit light curve\n",
    "2. **Preprocessing**: Applied cleaning and normalization\n",
    "3. **Visualization**: Created light curve and folded plots\n",
    "4. **Feature Extraction**: Computed 100+ astronomical features\n",
    "5. **Analysis**: Examined feature distributions and importance\n",
    "6. **Pruning**: Selected relevant features for analysis\n",
    "7. **Export**: Saved results to CSV files\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Use this workflow with your own `.npz` light curve files\n",
    "- Explore different feature selection strategies\n",
    "- Add custom features specific to your science case\n",
    "- Use the extracted features for machine learning classification or regression\n",
    "\n",
    "### Key Features Detected\n",
    "\n",
    "The analysis successfully detected:\n",
    "- Transit signals in the light curve\n",
    "- Correct period estimation (if Lomb-Scargle worked)\n",
    "- Variability characteristics\n",
    "- Statistical properties of the data\n",
    "\n",
    "This demonstrates the power of comprehensive feature extraction for astronomical time series analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}