# ðŸ—ï¸ UNIFIED EXOPLANET DETECTION SYSTEM - ARCHITECTURE DESIGN

**Version:** 2.0.0  
**Design Date:** October 26, 2025  
**Status:** Implementation Ready

---

## ðŸŽ¯ SYSTEM OVERVIEW

The **Unified Exoplanet Detection System** combines the best features from all branches into a single, cohesive, production-ready platform for detecting exoplanets from light curve data.

### Design Principles

1. **Modularity:** Each component is independent and replaceable
2. **Scalability:** Handles single files to thousands of light curves
3. **Flexibility:** Multiple feature extraction modes (fast/standard/comprehensive)
4. **Performance:** Optimized for modern multi-core processors
5. **Usability:** CLI, Python API, and web dashboard interfaces
6. **Extensibility:** Easy to add new models, features, or visualizations

---

## ðŸ“¦ PROJECT STRUCTURE

```
exoplanet_detection/
â”œâ”€â”€ README.md                          # Main documentation
â”œâ”€â”€ CHANGELOG.md                       # Version history
â”œâ”€â”€ LICENSE                            # MIT License
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ pyproject.toml                     # Modern Python packaging
â”œâ”€â”€ config.yaml                        # Default configuration
â”‚
â”œâ”€â”€ exodet/                           # Main package
â”‚   â”œâ”€â”€ __init__.py                   # Package initialization
â”‚   â”œâ”€â”€ __version__.py                # Version info
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                         # Core functionality
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py           # Unified data loading (NPZ/CSV)
â”‚   â”‚   â”œâ”€â”€ preprocessing.py          # Advanced preprocessing pipeline
â”‚   â”‚   â”œâ”€â”€ config.py                 # Configuration management
â”‚   â”‚   â””â”€â”€ exceptions.py             # Custom exceptions
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                     # Feature extraction
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ basic_extractor.py       # Fast basic features (100+)
â”‚   â”‚   â”œâ”€â”€ ml_extractor.py          # ML-optimized features (50+)
â”‚   â”‚   â”œâ”€â”€ tsfresh_extractor.py     # Comprehensive TSFresh (350+)
â”‚   â”‚   â”œâ”€â”€ feature_selector.py      # Feature selection & pruning
â”‚   â”‚   â””â”€â”€ feature_registry.py      # Feature catalog
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                       # Machine learning models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_model.py            # Abstract base class
â”‚   â”‚   â”œâ”€â”€ siamese_network.py       # Siamese neural network
â”‚   â”‚   â”œâ”€â”€ fcnn_classifier.py       # Fully connected classifier
â”‚   â”‚   â”œâ”€â”€ ensemble.py              # Ensemble methods
â”‚   â”‚   â””â”€â”€ hyperopt.py              # Hyperparameter optimization
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                     # Training pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py               # Training orchestrator
â”‚   â”‚   â”œâ”€â”€ pair_generator.py        # Siamese pair generation
â”‚   â”‚   â”œâ”€â”€ data_splitter.py         # Train/val/test splitting
â”‚   â”‚   â”œâ”€â”€ callbacks.py             # Training callbacks
â”‚   â”‚   â””â”€â”€ metrics.py               # Evaluation metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/                # Visualization tools
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ lightcurve_plots.py      # Light curve visualization
â”‚   â”‚   â”œâ”€â”€ feature_plots.py         # Feature analysis plots
â”‚   â”‚   â”œâ”€â”€ model_plots.py           # Model performance plots
â”‚   â”‚   â”œâ”€â”€ dashboard_components.py  # Dashboard widgets
â”‚   â”‚   â””â”€â”€ report_generator.py      # Automated reporting
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                        # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ file_utils.py            # File I/O operations
â”‚   â”‚   â”œâ”€â”€ math_utils.py            # Mathematical helpers
â”‚   â”‚   â”œâ”€â”€ parallel.py              # Parallel processing
â”‚   â”‚   â”œâ”€â”€ logger.py                # Logging configuration
â”‚   â”‚   â””â”€â”€ validation.py            # Data validation
â”‚   â”‚
â”‚   â””â”€â”€ cli/                          # Command-line interface
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ main.py                  # Main CLI entry point
â”‚       â”œâ”€â”€ commands/                # CLI commands
â”‚       â”‚   â”œâ”€â”€ extract.py           # Feature extraction command
â”‚       â”‚   â”œâ”€â”€ train.py             # Training command
â”‚       â”‚   â”œâ”€â”€ evaluate.py          # Evaluation command
â”‚       â”‚   â”œâ”€â”€ batch.py             # Batch processing command
â”‚       â”‚   â””â”€â”€ dashboard.py         # Launch dashboard command
â”‚       â””â”€â”€ args.py                  # Argument parsers
â”‚
â”œâ”€â”€ dashboard/                        # Web dashboard (Streamlit)
â”‚   â”œâ”€â”€ app.py                       # Main dashboard app
â”‚   â”œâ”€â”€ pages/                       # Multi-page dashboard
â”‚   â”‚   â”œâ”€â”€ 1_Data_Explorer.py      # Data exploration
â”‚   â”‚   â”œâ”€â”€ 2_Feature_Extraction.py  # Feature extraction interface
â”‚   â”‚   â”œâ”€â”€ 3_Model_Training.py     # Training interface
â”‚   â”‚   â”œâ”€â”€ 4_Model_Evaluation.py   # Evaluation interface
â”‚   â”‚   â””â”€â”€ 5_Batch_Processing.py   # Batch operations
â”‚   â””â”€â”€ components/                  # Reusable components
â”‚       â”œâ”€â”€ file_uploader.py
â”‚       â”œâ”€â”€ plot_viewer.py
â”‚       â””â”€â”€ metrics_display.py
â”‚
â”œâ”€â”€ scripts/                          # Standalone scripts
â”‚   â”œâ”€â”€ convert_to_excel.py          # CSV to Excel converter
â”‚   â”œâ”€â”€ clean_data.py                # Data cleaning utility
â”‚   â”œâ”€â”€ merge_datasets.py            # Dataset merging
â”‚   â””â”€â”€ benchmark_performance.py     # Performance testing
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_Quick_Start.ipynb        # Getting started guide
â”‚   â”œâ”€â”€ 02_Data_Exploration.ipynb   # Data analysis
â”‚   â”œâ”€â”€ 03_Feature_Engineering.ipynb # Feature development
â”‚   â”œâ”€â”€ 04_Model_Training.ipynb     # Training examples
â”‚   â””â”€â”€ 05_Advanced_Topics.ipynb    # Advanced techniques
â”‚
â”œâ”€â”€ tests/                            # Unit & integration tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data_loader.py
â”‚   â”œâ”€â”€ test_preprocessing.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_integration.py
â”‚
â”œâ”€â”€ docs/                             # Documentation
â”‚   â”œâ”€â”€ index.md                     # Documentation home
â”‚   â”œâ”€â”€ installation.md              # Installation guide
â”‚   â”œâ”€â”€ quickstart.md                # Quick start tutorial
â”‚   â”œâ”€â”€ user_guide/                  # User documentation
â”‚   â”‚   â”œâ”€â”€ data_preparation.md
â”‚   â”‚   â”œâ”€â”€ feature_extraction.md
â”‚   â”‚   â”œâ”€â”€ model_training.md
â”‚   â”‚   â””â”€â”€ visualization.md
â”‚   â”œâ”€â”€ api_reference/               # API documentation
â”‚   â”‚   â”œâ”€â”€ core.md
â”‚   â”‚   â”œâ”€â”€ features.md
â”‚   â”‚   â”œâ”€â”€ models.md
â”‚   â”‚   â””â”€â”€ visualization.md
â”‚   â””â”€â”€ developer_guide/             # Developer docs
â”‚       â”œâ”€â”€ contributing.md
â”‚       â”œâ”€â”€ architecture.md
â”‚       â””â”€â”€ extending.md
â”‚
â”œâ”€â”€ examples/                         # Example code
â”‚   â”œâ”€â”€ basic_pipeline.py            # Simple example
â”‚   â”œâ”€â”€ batch_processing.py          # Batch example
â”‚   â”œâ”€â”€ custom_features.py           # Custom feature example
â”‚   â””â”€â”€ hyperparameter_tuning.py    # Optimization example
â”‚
â”œâ”€â”€ data/                             # Data directory (not in git)
â”‚   â”œâ”€â”€ raw/                         # Raw NPZ/CSV files
â”‚   â”œâ”€â”€ processed/                   # Processed data
â”‚   â”œâ”€â”€ features/                    # Extracted features
â”‚   â””â”€â”€ models/                      # Saved models
â”‚
â””â”€â”€ outputs/                          # Output directory (not in git)
    â”œâ”€â”€ logs/                        # Log files
    â”œâ”€â”€ plots/                       # Generated plots
    â”œâ”€â”€ reports/                     # Generated reports
    â””â”€â”€ checkpoints/                 # Model checkpoints
```

---

## ðŸ”§ COMPONENT SPECIFICATIONS

### 1. Core Module (`exodet/core/`)

#### `data_loader.py`
```python
class UniversalDataLoader:
    """Load data from NPZ, CSV, or FITS files"""
    
    def load_npz(file_path) -> LightCurve
    def load_csv(file_path) -> LightCurve
    def load_batch(directory) -> List[LightCurve]
    def auto_detect_format(file_path) -> str
```

#### `preprocessing.py`
```python
class PreprocessingPipeline:
    """Configurable preprocessing pipeline"""
    
    def remove_nans()
    def detrend(method='polynomial', order=3)
    def sigma_clip(sigma=3, iterations=3)
    def normalize(method='zscore')
    def fold(period, epoch)
    def bin_data(bin_size)
```

#### `config.py`
```python
class Config:
    """Unified configuration management"""
    
    def load_from_yaml(path)
    def load_from_dict(config_dict)
    def validate()
    def save(path)
```

### 2. Features Module (`exodet/features/`)

#### Three-Tier Feature Extraction

**Tier 1: Fast (100+ features, <1s per curve)**
- Basic statistics
- Simple time-domain features
- Quick transit detection
- Use case: Initial screening, large datasets

**Tier 2: Standard (150+ features, 2-5s per curve)**
- All Tier 1 features
- Advanced statistical features
- Frequency-domain analysis
- ML-optimized features
- Use case: Standard analysis, model training

**Tier 3: Comprehensive (500+ features, 10-30s per curve)**
- All Tier 1 & 2 features
- Full TSFresh feature set
- Advanced astronomical features
- Use case: Final analysis, research

#### `feature_registry.py`
```python
class FeatureRegistry:
    """Central registry of all available features"""
    
    features: Dict[str, FeatureDefinition]
    
    def register(name, function, category, tier)
    def get_by_tier(tier) -> List[Feature]
    def get_by_category(category) -> List[Feature]
    def extract(data, features_list) -> DataFrame
```

### 3. Models Module (`exodet/models/`)

#### `siamese_network.py`
```python
class SiameseNetwork(BaseModel):
    """Siamese neural network for similarity learning"""
    
    architecture:
        - Feature Extractor: [256, 128, 64]
        - Embedding Layer: 32D
        - Distance Metric: Euclidean
        - Loss: Contrastive Loss
    
    def forward(x1, x2)
    def get_embedding(x)
    def compute_distance(x1, x2)
    def predict_similarity(x1, x2)
```

#### `ensemble.py`
```python
class EnsembleDetector:
    """Ensemble of multiple models"""
    
    models: List[BaseModel]
    
    def add_model(model, weight)
    def predict_ensemble(data)
    def optimize_weights(validation_data)
```

### 4. Training Module (`exodet/training/`)

#### `trainer.py`
```python
class Trainer:
    """Unified training orchestrator"""
    
    def train(model, train_data, val_data, config)
    def evaluate(model, test_data)
    def hyperparameter_search(search_space, n_trials)
    def save_checkpoint(epoch, model, optimizer)
    def load_checkpoint(path)
```

### 5. Visualization Module (`exodet/visualization/`)

#### Plot Types
- Light curve plots (raw, processed, folded)
- Feature distribution plots
- Feature correlation heatmaps
- Model performance curves (loss, metrics)
- Confusion matrices
- ROC curves
- t-SNE embeddings
- Comprehensive analysis dashboards

### 6. CLI Module (`exodet/cli/`)

#### Command Structure
```bash
exodet <command> [options]

Commands:
  extract     Extract features from light curves
  train       Train a detection model
  evaluate    Evaluate model performance
  predict     Make predictions on new data
  batch       Batch process multiple files
  dashboard   Launch web dashboard
  version     Show version info
  config      Configuration management
```

---

## ðŸŽ¨ USER INTERFACES

### 1. Command-Line Interface (CLI)

**Example Usage:**
```bash
# Extract features (fast mode)
exodet extract --input data/raw/*.npz --output features.csv --mode fast

# Extract comprehensive features with parallel processing
exodet extract --input data/raw/ --output features.csv --mode comprehensive --workers 8

# Train Siamese network
exodet train --features features.csv --model siamese --config config.yaml

# Batch processing with visualization
exodet batch --input data/raw/ --output results/ --visualize --workers 4

# Launch dashboard
exodet dashboard --port 8501
```

### 2. Python API

**Example Usage:**
```python
from exodet import ExoplanetDetector
from exodet.features import FeatureExtractor
from exodet.models import SiameseNetwork

# Initialize detector
detector = ExoplanetDetector(config='config.yaml')

# Load and preprocess data
lc = detector.load('lightcurve.npz')
lc_clean = detector.preprocess(lc, detrend=True, sigma_clip=True)

# Extract features (multi-tier)
extractor = FeatureExtractor(tier='standard')
features = extractor.extract(lc_clean)

# Train model
model = SiameseNetwork()
detector.train(model, features, epochs=100)

# Evaluate
metrics = detector.evaluate(model, test_data)

# Predict
prediction = detector.predict('new_lightcurve.npz')
```

### 3. Web Dashboard (Streamlit)

**Pages:**
1. **Data Explorer** - Upload, visualize, and explore light curves
2. **Feature Extraction** - Interactive feature extraction with preview
3. **Model Training** - Configure and train models with live monitoring
4. **Model Evaluation** - Comprehensive evaluation with visualizations
5. **Batch Processing** - Process multiple files with progress tracking

---

## âš™ï¸ CONFIGURATION SYSTEM

### `config.yaml` Structure

```yaml
# System Configuration
system:
  seed: 42
  device: auto  # auto, cpu, cuda
  num_workers: 4
  verbose: true
  log_level: INFO

# Data Configuration
data:
  input_dir: data/raw/
  output_dir: data/processed/
  file_format: auto  # auto, npz, csv, fits
  
# Preprocessing Configuration
preprocessing:
  remove_nans: true
  detrend:
    enabled: true
    method: polynomial  # polynomial, savgol, gp
    order: 3
  sigma_clip:
    enabled: true
    sigma: 3.0
    iterations: 3
  normalize:
    enabled: true
    method: zscore  # zscore, minmax, robust
  fold:
    enabled: false
    period: null
    epoch: 0
  bin:
    enabled: false
    bin_size: 0.01

# Feature Extraction Configuration
features:
  tier: standard  # fast, standard, comprehensive
  custom_features: []
  parallel: true
  cache_enabled: true
  
  # Tier-specific settings
  fast:
    timeout: 1.0  # seconds per curve
  standard:
    timeout: 5.0
  comprehensive:
    timeout: 30.0
    tsfresh_params: ComprehensiveFCParameters

# Model Configuration
model:
  type: siamese  # siamese, fcnn, ensemble
  architecture:
    hidden_dims: [256, 128, 64]
    embedding_dim: 32
    dropout_rate: 0.3
    activation: relu
  
# Training Configuration
training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  weight_decay: 0.00001
  optimizer: adam  # adam, sgd, rmsprop
  scheduler:
    enabled: true
    patience: 5
    factor: 0.5
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.0001
  
# Evaluation Configuration
evaluation:
  metrics: [accuracy, precision, recall, f1, auc]
  visualizations: [confusion_matrix, roc_curve, embedding]
  save_predictions: true

# Output Configuration
output:
  save_plots: true
  save_features: true
  save_models: true
  format: csv  # csv, excel, hdf5
  compression: gzip
```

---

## ðŸš€ PERFORMANCE OPTIMIZATION STRATEGIES

### 1. Parallel Processing
- Multi-core feature extraction
- Batch data loading
- Parallel model inference

### 2. Memory Management
- Lazy loading for large datasets
- Memory-mapped file I/O
- Garbage collection optimization
- Configurable batch sizes

### 3. Caching
- Feature extraction caching
- Preprocessed data caching
- Model checkpoint caching

### 4. Code Optimization
- Vectorized operations (NumPy)
- JIT compilation where applicable
- Efficient data structures

---

## ðŸ”’ QUALITY ASSURANCE

### 1. Testing Strategy
- Unit tests (>80% coverage)
- Integration tests
- Performance benchmarks
- Regression tests

### 2. Code Quality
- Type hints throughout
- Comprehensive docstrings
- PEP 8 compliance
- Code review process

### 3. Documentation
- API reference (auto-generated)
- User guide (comprehensive)
- Developer guide
- Example gallery

---

## ðŸ“Š EXPECTED PERFORMANCE

### Processing Speed
- Fast Mode: 1000+ curves/hour (single core)
- Standard Mode: 500+ curves/hour (single core)
- Comprehensive Mode: 100+ curves/hour (single core)
- With 8 cores: 5x-8x speedup

### Model Performance
- Accuracy: 85-92%
- Precision: 80-88%
- Recall: 75-85%
- F1-Score: 78-86%
- ROC-AUC: 0.88-0.94

### Resource Usage
- Memory: 2-8 GB (depending on mode)
- CPU: Scales with cores
- Storage: 1-10 MB per processed curve

---

## ðŸ”„ MIGRATION PATH

### From Main Branch
1. Import existing preprocessing pipeline
2. Integrate visualization tools
3. Port CLI commands
4. Adapt batch processing

### From newcode
1. Import Siamese network architecture
2. Integrate training pipeline
3. Port configuration system
4. Adapt dashboard components

### From newcode2
1. Import TSFresh extractor
2. Integrate performance optimizations
3. Port parallel processing logic
4. Adapt caching mechanisms

### From newcode3
1. Import utility scripts
2. Integrate Excel export
3. Port modern packaging

---

## ðŸŽ¯ SUCCESS CRITERIA

### Functionality
âœ… Supports NPZ, CSV, and FITS formats  
âœ… Three-tier feature extraction (fast/standard/comprehensive)  
âœ… Multiple ML models (Siamese, FCNN, ensemble)  
âœ… Comprehensive visualization suite  
âœ… CLI, Python API, and web dashboard

### Performance
âœ… Process 500+ curves/hour (standard mode)  
âœ… 85%+ accuracy on validation data  
âœ… <8 GB memory usage  
âœ… Scales to 8+ cores

### Usability
âœ… Single command installation  
âœ… <5 minutes to first prediction  
âœ… Comprehensive documentation  
âœ… Rich example gallery

### Quality
âœ… 80%+ test coverage  
âœ… PEP 8 compliant  
âœ… Full type hints  
âœ… No critical bugs

---

## ðŸ“… IMPLEMENTATION TIMELINE

**Total Estimated Time: 4-5 weeks**

### Week 1: Foundation
- Project structure setup
- Core modules (data loader, preprocessing, config)
- Basic testing framework

### Week 2: Feature Extraction
- Three-tier feature extraction
- Feature registry and selection
- Performance optimization

### Week 3: Machine Learning
- Model implementations
- Training pipeline
- Hyperparameter optimization

### Week 4: Interfaces & Visualization
- CLI implementation
- Dashboard development
- Visualization tools

### Week 5: Polish & Documentation
- Comprehensive testing
- Documentation writing
- Examples and tutorials
- Performance optimization

---

**Status:** Ready for implementation  
**Next Action:** Begin core module development

